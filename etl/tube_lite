#!/usr/bin/env python3

"""A lightweight replacement for gen3's spark/tube."""

from datetime import datetime

import click
import requests
from gen3.auth import Gen3Auth
from gen3.submission import Gen3Submission
from jsonpath_ng import parse

"""
export EP='--endpoint https://aced-training.compbio.ohsu.edu'
# program-project - the program & project we want to load, see https://aced-training.compbio.ohsu.edu/submission
# credentials from https://aced-training.compbio.ohsu.edu/identity
export CRED='--credentials_path  ./Secrets/credentials-etl.json'
# DEBUGGING
export ETL="python3 ./scripts/etl_lite.py $EP $CRED --batch_size 100 --output_path /tmp/etl_lite.output.txt"
# PROD
export ETL="python3 ./scripts/etl_lite.py $EP $CRED --batch_size 100 --elastic http://esproxy-service:9200"
$ETL

"""

DEFAULT_ELASTIC = "http://esproxy-service:9200"
DEFAULT_NAMESPACE = "gen3.aced.io"

# gen3 graph-model query

# graphql for FILE

FILE_PROPERTIES = """
    file_id: id
    category_0_coding_0_code
    category_0_coding_0_display
    category_0_coding_0_system
    content_0_attachment_contentType
    content_0_attachment_creation
    content_0_attachment_data
    content_0_attachment_hash
    content_0_attachment_language
    content_0_attachment_size
    content_0_attachment_title
    content_0_attachment_url
    content_0_format_code
    content_0_format_display
    content_0_format_system
    context_encounter_0_reference
    context_encounter_0_type
    context_event_0_coding_0_code
    context_event_0_coding_0_display
    context_event_0_coding_0_system
    context_facilityType_coding_0_code
    context_facilityType_coding_0_display
    context_facilityType_coding_0_system
    context_period_end
    context_period_start
    context_practiceSetting_coding_0_code
    context_practiceSetting_coding_0_display
    context_practiceSetting_coding_0_system
    context_related_0_reference
    context_related_0_type
    context_sourcePatientInfo_reference
    context_sourcePatientInfo_type
    created_datetime
    data_category
    data_format
    data_type
    date
    description
    docStatus
    error_type
    file_format
    file_name
    file_size
    file_state
    ga4gh_drs_uri
    id
    identifier_0_system
    identifier_0_use
    identifier_0_value
    language
    masterIdentifier_system
    masterIdentifier_use
    masterIdentifier_value
    md5sum
    object_id
    project_id
    file_resource_type: resource_type
    securityLabel_0_coding_0_code
    securityLabel_0_coding_0_display
    securityLabel_0_coding_0_system
    state
    status
    file_submitter_id: submitter_id
    type_coding_0_code
    type_coding_0_display
    type_coding_0_system
"""


PATIENT_PROPERTIES = """
        patient_id: id
        patient_submitter_id: submitter_id
        active
        birthDate
        communication_0_language_coding_0_code
        communication_0_language_coding_0_display
        communication_0_language_coding_0_system
        communication_0_preferred
        contact_0_gender
        contact_0_organization_reference
        contact_0_organization_type
        contact_0_period_end
        contact_0_period_start
        contact_0_relationship_0_coding_0_code
        contact_0_relationship_0_coding_0_display
        contact_0_relationship_0_coding_0_system
        created_datetime
        deceasedBoolean
        deceasedDateTime
        gender
        id
        identifier_0_system
        identifier_0_use
        identifier_0_value
        language
        link_0_other_reference
        link_0_other_type
        link_0_type
        maritalStatus_coding_0_code
        maritalStatus_coding_0_display
        maritalStatus_coding_0_system
        multipleBirthBoolean
        multipleBirthInteger
        photo_0_contentType
        photo_0_creation
        photo_0_data
        photo_0_hash
        photo_0_language
        photo_0_size
        photo_0_title
        photo_0_url
        project_id
        patient_resource_type: resource_type
        state
        submitter_id
      
"""
FILE_GRAPHQL = """
query ($first: Int!, $offset: Int!) {
 document_reference(first: $first, offset: $offset)  {
    $FILE_PROPERTIES
    subject_patient{
        $PATIENT_PROPERTIES
    }
  }
}
""".replace('$FILE_PROPERTIES', FILE_PROPERTIES)\
    .replace('$PATIENT_PROPERTIES', PATIENT_PROPERTIES)

#   graphql query for CASE
PATIENT_GRAPHQL = """
query ($first: Int!, $offset: Int!) {
  patient(first: $first, offset: $offset) {
    $PATIENT_PROPERTIES
    document_references {
        $FILE_PROPERTIES
    }
  }
}
""".replace('$PATIENT_PROPERTIES', PATIENT_PROPERTIES)\
    .replace('$FILE_PROPERTIES', FILE_PROPERTIES)


def key_values(nested, key, normalize=True):
    """Return all values of a key in nested, cast to None, and scalar if normalize"""
    jsonpath_expression = parse(f"$..[{key}]")
    matches = jsonpath_expression.find(nested)
    if normalize:
        if len(matches) == 0:
            return None
        if len(matches) == 1:
            return matches[0].value
    return [match.value for match in matches]


def sum_key_values(nested, key):
    """Return the sum of values for key."""
    return sum(key_values(nested, key, normalize=False))


def create_index_from_source(_source, _index, _type):
    """Given an ES source dict, create ES index."""
    mappings = {}
    for k, v in _source.items():
        if type(v).__name__ in ['str', 'NoneType']:
            mappings[k] = {
                "type": "keyword"
            }
        elif isinstance(v, list):
            mappings[k] = {
                "type": "text"
            }
        else:
            # naive, there are probably other types
            mappings[k] = {"type": "float"}
    return {
        "mappings": {_type: {"properties": mappings}}
    }


def submission_client(endpoint, refresh_file):
    """Create authorized client."""
    auth = Gen3Auth(endpoint, refresh_file=refresh_file)
    assert auth, 'should return an auth client'
    submission_client_ = Gen3Submission(endpoint, auth)
    assert submission_client_, 'should return a submission client'
    assert 'delete_program' in dir(submission_client_), 'missing delete_program'
    assert 'create_program' in dir(submission_client_), 'missing create_program'
    return submission_client_


def drop_file_indexes(elastic=DEFAULT_ELASTIC, name_space=DEFAULT_NAMESPACE):
    """Drop the es indexes."""
    return {"method": 'DELETE', "url": f'{elastic}/{name_space}_file_0'}


def write_array_aliases(elastic=DEFAULT_ELASTIC, name_space=DEFAULT_NAMESPACE):
    """Write the array aliases."""
    # EXPECTED_ALIASES = {
    #     ".kibana_1": {
    #         "aliases": {
    #             ".kibana": {}
    #         }
    #     },
    #     "etl-array-config_0": {
    #         "aliases": {
    #             "etl-array-config": {},
    #             "etl_array-config": {},
    #             "time_2022-08-25T01:44:47.115494": {}
    #         }
    #     },
    #     "etl_0": {
    #         "aliases": {
    #             "etl": {},
    #             "time_2022-08-25T01:44:47.115494": {}
    #         }
    #     },
    #     "file-array-config_0": {
    #         "aliases": {
    #             "file-array-config": {},
    #             "file_array-config": {},
    #             "time_2022-08-25T01:44:47.115494": {}
    #         }
    #     },
    #     "file_0": {
    #         "aliases": {
    #             "file": {},
    #             "time_2022-08-25T01:44:47.115494": {}
    #         }
    #     }
    # }
    return {
        "method": 'POST',
        "url": f'{elastic}/_aliases',
        "json": {
            "actions": [{"add": {"index": f"{name_space}_file-array-config_0",
                                 "alias": f"{name_space}_array-config"}},
                        {"add": {"index": f"{name_space}_case-array-config_0",
                                 "alias": f"{name_space}_array-config"}},
                        {"add": {"index": f"{name_space}_file-array-config_0",
                                 "alias": f"file_array-config"}},
                        {"add": {"index": f"{name_space}_case-array-config_0",
                                 "alias": f"etl_array-config"}}
                        ]}
    }


def create_file_indexes(_source, elastic=DEFAULT_ELASTIC, name_space=DEFAULT_NAMESPACE):
    """Create the es indexes."""
    _index = f"{name_space}_file_0"
    _type = "file"
    return {
        "method": 'PUT',
        "url": f'{elastic}/{_index}',
        "json": create_index_from_source(_source, _index, _type)
    }


def write_file_array_config(elastic=DEFAULT_ELASTIC, name_space=DEFAULT_NAMESPACE):
    """Write the array config."""
    return {
        "method": 'PUT',
        "url": f'{elastic}/{name_space}_file-array-config_0/_doc/file',
        "json": {"timestamp": datetime.now().isoformat(), "array": ["project_code", "program_name"]}
    }


# gen3.aced.io_array-config gen3.aced.io_file-array-config_0 - - -

# case                      gen3.aced.io_case_0              - - -
# etl                       gen3.aced.io_case_0              - - -
# file                      gen3.aced.io_file_0              - - -

# gen3.aced.io_case         gen3.aced.io_case_0              - - -
# gen3.aced.io_file         gen3.aced.io_file_0              - - -
# etl_array-config          gen3.aced.io_case-array-config_0 - - -
# gen3.aced.io_array-config gen3.aced.io_case-array-config_0 - - -
# .kibana                   .kibana_1                        - - -


def write_file_alias_config(elastic=DEFAULT_ELASTIC, name_space=DEFAULT_NAMESPACE):
    """Write the alias config."""
    return {
        "method": 'POST',
        "url": f'{elastic}/_aliases',
        "json": {"actions": [{"add": {"index": f"{name_space}_file_0", "alias": f"file"}}]}  # {name_space}_
    }


def write_file_data(row, elastic=DEFAULT_ELASTIC, name_space=DEFAULT_NAMESPACE):
    """Write data."""
    return {
        "method": 'POST',
        "url": f'{elastic}/{name_space}_file_0/file',
        "json": row
    }


def drop_patient_indexes(elastic=DEFAULT_ELASTIC, name_space=DEFAULT_NAMESPACE):
    """Drop the es indexes."""
    return {"method": 'DELETE', "url": f'{elastic}/{name_space}_case_0'}


def create_patient_indexes(_source, elastic=DEFAULT_ELASTIC, name_space=DEFAULT_NAMESPACE):
    """Create the es indexes."""
    _index = f"{name_space}_case_0"
    _type = "case"
    return {
        "method": 'PUT',
        "url": f'{elastic}/{_index}',
        "json": create_index_from_source(_source, _index, _type)
    }


def write_patient_array_config(elastic=DEFAULT_ELASTIC, name_space=DEFAULT_NAMESPACE):
    """Write the array config."""
    # { "timestamp": "2021-09-18T17:10:49.107081", "array": [ "_file_id" ] }
    return {
        "method": 'PUT',
        "url": f'{elastic}/{name_space}_case-array-config_0/_doc/etl',
        "json": {"timestamp": datetime.now().isoformat(), "array": ['data_format', 'data_type', '_file_id']}
    }


def write_patient_alias_config(elastic=DEFAULT_ELASTIC, name_space=DEFAULT_NAMESPACE):
    """Write the alias config."""
    return {
        "method": 'POST',
        "url": f'{elastic}/_aliases',
        "json": {"actions": [{"add": {"index": f"{name_space}_case_0", "alias": f"etl"}}]}
    }


def write_patient_data(row, elastic=DEFAULT_ELASTIC, name_space=DEFAULT_NAMESPACE):
    """Write data."""
    return {
        "method": 'POST',
        "url": f'{elastic}/{name_space}_case_0/case',
        "json": row
    }


def read_files(sc, batch_size):
    """Read file records and their ancestors from gen3, map to elastic search."""
    first = batch_size
    offset = 0
    c = 0
    while True:
        graphql_variables = {"first": first, "offset": offset}
        graph_ql_response = sc.query(FILE_GRAPHQL, variables=graphql_variables)
        c += 1
        has_data = False

        if 'data' in graph_ql_response:
            sum_data = 0
            for k in graph_ql_response['data']:
                sum_data += len(graph_ql_response['data'][k])
            has_data = sum_data > 0
        if not has_data:
            break
        offset += batch_size
        for file_entity in graph_ql_response['data']:
            for f in graph_ql_response['data'][file_entity]:
                assert 'project_id' in f, (file_entity, f.keys())
                program, project = f['project_id'].split('-')

                source_ = {
                    "file_id": f['file_id'],
                    # "author_display": f['author_display'],
                    # "author_reference": f['author_reference'],
                    "auth_resource_path": f"/programs/{program}/projects/{project}",
                    # "category_coding_us_core_documentreference_category": f['category_coding_us_core_documentreference_category'],

                    "content_attachment_contentType": f['content_0_attachment_contentType'],
                    # "content_attachment_md5": f['content_0_attachment_md5'],  TODO ?
                    "content_attachment_size": f['content_0_attachment_size'],
                    "content_attachment_url": f['content_0_attachment_url'],
                    "content_attachment_url": f['category_0_coding_0_system'],

                    "content_format_code": f['content_0_format_code'],
                    "content_format_system": f['content_0_format_system'],
                    "content_format_display": f['content_0_format_display'],

                    "context_encounter_reference": f['context_encounter_0_reference'],
                    "context_period_end": f['context_period_end'],
                    "context_period_start": f['context_period_start'],
                    "date": f['date'],
                    "ga4gh_drs_uri": f['ga4gh_drs_uri'],
                    # "identifier_urn_ietf_rfc_3986": f['identifier_urn_ietf_rfc_3986'],
                    "file_resource_type": f['context_encounter_0_reference'],
                    "status": f['status'],
                    "subject_patient": f['subject_patient'][0]['id'],
                    "submitter_id": f['file_submitter_id'],

                    "type_coding_code": f['type_coding_0_code'],
                    "type_coding_display": f['type_coding_0_display'],
                    "type_coding_system": f['type_coding_0_system'],

                    # "type_coding_1_code": f['type_coding_1_code'],
                    # "type_coding_1_system": f['type_coding_1_system'],
                    # "type_coding_1_display": f['type_coding_1_display'],

                    "md5sum": f['md5sum'],
                    'project_id': f['project_id'],
                    'object_id': f['object_id'],
                    'data_format': f['data_format'],
                    'file_name': f['file_name'],
                    'data_type': f['data_type'],
                    'file_size': f['file_size'],
                    'state': f['state'],

                    'project_code': [
                        project
                    ],
                    'program_name': [
                        program
                    ]

                }

                yield source_


def read_patients(sc, batch_size):
    """Read patient records and their descendants from gen3."""
    first = batch_size
    offset = 0
    c = 0
    while True:
        # pagination
        graphql_variables = {"first": first, "offset": offset}
        r = sc.query(PATIENT_GRAPHQL, variables=graphql_variables)
        c += 1

        if 'data' not in r or 'patient' not in r['data'] or len(r['data']['patient']) == 0:
            break
        offset += batch_size
        for patient in r['data']['patient']:
            program, project = patient['project_id'].split('-')
            _source = {
                "project_id": patient['project_id'],
                "patient_id": patient['id'],
                "patient_resource_type": patient['patient_resource_type'],
                # "text_status": patient['text_status'],
                # "text_div": patient['text_div'],

                # TODO - review extension anonymization
                # "us_core_race_ombCategory": patient['us_core_race_ombCategory'],
                # "us_core_race_text": patient['us_core_race_text'],
                # "us_core_ethnicity_ombCategory": patient['us_core_ethnicity_ombCategory'],
                # "us_core_ethnicity_text": patient['us_core_ethnicity_text'],
                # "patient_mothersMaidenName": patient['patient_mothersMaidenName'],
                # "us_core_birthsex": patient['us_core_birthsex'],

                # TODO - more extension
                # "patient_birthPlace_city": patient['patient_birthPlace_city'],
                # "patient_birthPlace_state": patient['patient_birthPlace_state'],
                # "patient_birthPlace_country": patient['patient_birthPlace_country'],

                # TODO - more extension
                # "disability_adjusted_life_years": patient['disability_adjusted_life_years'],
                # "quality_adjusted_life_years": patient['quality_adjusted_life_years'],
                # "identifier_synthea": patient['identifier_synthea'],
                # "identifier_MR": patient['identifier_MR'],
                # "identifier_SS": patient['identifier_SS'],
                # "identifier_DL": patient['identifier_DL'],
                # "identifier_PPN": patient['identifier_PPN'],

                # TODO - anonymizer
                # "name_use": patient['name_use'],
                # "name_family": patient['name_family'],
                # "name_given": patient['name_given'],
                # "name_prefix": patient['name_prefix'],
                # "telecom_system": patient['telecom_system'],
                # "telecom_value": patient['telecom_value'],
                # "telecom_use": patient['telecom_use'],

                "gender": patient['gender'],
                "birthDate": patient['birthDate'],
                "deceasedDateTime": patient['deceasedDateTime'],
                "deceasedBoolean": patient['deceasedBoolean'],

                # TODO - anonymizer
                # "address_geolocation_latitude": patient['address_geolocation_latitude'],
                # "address_geolocation_longitude": patient['address_geolocation_longitude'],
                # "address_line": patient['address_line'],
                # "address_city": patient['address_city'],
                # "address_state": patient['address_state'],
                # "address_postalCode": patient['address_postalCode'],
                # "address_country": patient['address_country'],

                "maritalStatus_coding_0_code": patient['maritalStatus_coding_0_code'],
                "maritalStatus_coding_0_system": patient['maritalStatus_coding_0_system'],
                "maritalStatus_coding_0_display": patient['maritalStatus_coding_0_display'],

                "multipleBirthBoolean": str(patient['multipleBirthBoolean']),   # TODO handle booleans better
                "multipleBirthInteger": patient['multipleBirthInteger'],

                # "communication_language_text": patient['communication_language_text'],
                "patient_submitter_id": patient['submitter_id'],
                "auth_resource_path": f"/programs/{program}/projects/{project}",

                "_case_id": patient['patient_id'],  # TODO ??? in gitops ?

                "file_id": key_values(patient, 'file_id'),
                "author_display": key_values(patient, 'author_display'),
                "author_reference": key_values(patient, 'author_reference'),
                # "category_coding_us_core_documentreference_category": key_values(patient, 'category_coding_us_core_documentreference_category'),

                "content_attachment_contentType": key_values(patient, 'content_0_attachment_contentType'),
                "content_attachment_md5": key_values(patient, 'content_0_attachment_md5'),
                "content_attachment_size": key_values(patient, 'content_0_attachment_size'),
                "content_attachment_url": key_values(patient, 'content_0_attachment_url'),

                "content_format_code": key_values(patient, 'content_0_format_code'),
                "content_format_system": key_values(patient, 'content_0_format_system'),
                "content_format_display": key_values(patient, 'content_0_format_display'),

                "context_encounter_reference": key_values(patient, 'context_encounter_0_reference'),
                "context_period_end": key_values(patient, 'context_period_end'),
                "context_period_start": key_values(patient, 'context_period_start'),

                # "custodian_display": key_values(patient, 'custodian_display'),
                # "custodian_reference": key_values(patient, 'custodian_reference'),
                #
                "date": key_values(patient, 'date'),
                "md5sum": key_values(patient, 'md5sum'),
                "ga4gh_drs_uri": key_values(patient, 'ga4gh_drs_uri'),
                # "identifier_urn_ietf_rfc_3986": key_values(patient, 'identifier_urn_ietf_rfc_3986'),

                "file_resourceType": key_values(patient, 'resource_type'),
                "status": key_values(patient, 'status'),
                # "file_submitter_id": key_values(patient, 'file_submitter_id'),
                # "type_coding_0_code": key_values(patient, 'type_coding_0_code'),
                # "type_coding_0_display": key_values(patient, 'type_coding_0_display'),
                # "type_coding_0_system": key_values(patient, 'type_coding_0_system'),
                # "type_coding_1_code": key_values(patient, 'type_coding_1_code'),
                # "type_coding_1_system": key_values(patient, 'type_coding_1_system'),
                # "type_coding_1_display": key_values(patient, 'type_coding_1_display'),
                "data_format": key_values(patient, 'data_format'),
                "data_type": key_values(patient, 'data_type')


            }

            yield _source


def write_dict(output, d):
    """Write a dict to the output."""
    output.write(str(d))
    output.write("\n")

def write_http(session, _params, raise_for_status=True):
    """Write a dict to the session."""
    r = session.request(**_params)
    if raise_for_status:
        if r.status_code > 300:
            print("TEXT_PARAMS", _params)
            print("TEXT", r.text)
        r.raise_for_status()


@click.command()
@click.option('--endpoint', type=str, help='Gen3 host base url.')
@click.option('--credentials_path', type=str, help='Path to gen3 credentials.')
@click.option('--batch_size', type=int, default=500, help='Number of records to read from gen3 at a time (500.')
@click.option('--output_path', type=str, default=None, help='For debugging, write the output to this path')
@click.option('--elastic', type=str, default=None, help='Write directly to elastic host')
def etl(credentials_path, endpoint, output_path, batch_size, elastic):
    """Extract file centric index from Gen3, create elastic search index."""
    # check destination
    assert output_path or elastic, "Please set either elastic url or output_path file path"
    # connect to source (gen3)
    sc = submission_client(endpoint, credentials_path)

    # create a handy little function that writes to either file or session
    output_stream = None
    write_method = None
    if output_path:
        output_stream = open(output_path, "w")
        write_method = write_dict
    else:
        output_stream = requests.sessions.Session()
        write_method = write_http

    def _writer(data):
        """Write to destination"""
        write_method(output_stream, data)

    #
    # FILE centric index
    #

    # assumes guppy-setup dropped ES indices
    #_writer(drop_file_indexes(elastic))
    # write data

    # for i, row in enumerate(read_files(sc, batch_size)):
    #     if i == 0:
    #         _writer(create_file_indexes(row, elastic))
    #     _writer(write_file_data(row, elastic))
    # _writer(write_file_array_config(elastic))
    # _writer(write_file_alias_config(elastic))
    # _writer(write_file_array_config(elastic))

    #
    # PATIENT centric index
    #

    for i, row in enumerate(read_patients(sc, batch_size)):
        if i == 0:
            _writer(create_patient_indexes(row, elastic))
        _writer(write_patient_data(row, elastic))
    _writer(write_patient_array_config(elastic))
    _writer(write_patient_alias_config(elastic))
    _writer(write_array_aliases(elastic))

    # cleanup
    output_stream.close()


if __name__ == '__main__':
    etl()
