#!/usr/bin/env python3

"""A lightweight replacement for gen3's spark/tube."""

from datetime import datetime

import click
import requests
from gen3.auth import Gen3Auth
from gen3.submission import Gen3Submission
from jsonpath_ng import parse

"""
export EP='--endpoint https://aced-training.compbio.ohsu.edu'
# program-project - the program & project we want to load, see https://aced-training.compbio.ohsu.edu/submission
# credentials from https://aced-training.compbio.ohsu.edu/identity
export CRED='--credentials_path  ./Secrets/credentials-etl.json'
# DEBUGGING
export ETL="python3 ./scripts/etl_lite.py $EP $CRED --batch_size 100 --output_path /tmp/etl_lite.output.txt"
# PROD
export ETL="python3 ./scripts/etl_lite.py $EP $CRED --batch_size 100 --elastic http://esproxy-service:9200"
$ETL

"""

DEFAULT_ELASTIC = "http://esproxy-service:9200"
DEFAULT_NAMESPACE = "gen3.aced.io"

# gen3 graph-model query

# graphql for FILE

# performance improvement: only retrieve what we need in the index (coordinate with gitops.json)
FILE_PROPERTIES = """
    file_id: id
    project_id
    data_type
    data_format
    file_name
    file_size
    object_id
    ga4gh_drs_uri
    date
    md5sum
    state
    submitter_id    
    file_category: category_0_coding_0_display
    content_0_attachment_url
    content_0_attachment_size
"""

# performance improvement: only retrieve what we need in the index (coordinate with gitops.json)
PATIENT_PROPERTIES = """
    id
    submitter_id
    project_id
    gender
    maritalStatus_coding_0_display
    deceasedBoolean
    deceasedDateTime
    birthDate
    patient_resource_type: resource_type  
    submitter_id
    ombCategory: extension_0_extension_0_valueCoding_display
    ombCategory_detail: extension_1_extension_0_valueCoding_display
    us_core_birthsex: extension_3_valueCode
    disability_adjusted_life_years: extension_5_valueDecimal
    quality_adjusted_life_years: extension_6_valueDecimal
    
"""

FILE_GRAPHQL = """
query ($first: Int!, $offset: Int!) {
    patient(first: $first, offset: $offset)  {
        $PATIENT_PROPERTIES
        document_references(first:1000) {
            $FILE_PROPERTIES            
        }
    }
}
""".replace('$FILE_PROPERTIES', FILE_PROPERTIES)\
    .replace('$PATIENT_PROPERTIES', PATIENT_PROPERTIES)

#   graphql query for CASE
PATIENT_GRAPHQL = """
query ($first: Int!, $offset: Int!) {
  patient(first: $first, offset: $offset) {
    $PATIENT_PROPERTIES
  }
}
""".replace('$PATIENT_PROPERTIES', PATIENT_PROPERTIES)\
    .replace('$FILE_PROPERTIES', FILE_PROPERTIES)


def key_values(nested, key, normalize=True):
    """Return all values of a key in nested, cast to None, and scalar if normalize"""
    jsonpath_expression = parse(f"$..[{key}]")
    matches = jsonpath_expression.find(nested)
    if normalize:
        if len(matches) == 0:
            return None
        if len(matches) == 1:
            return matches[0].value
    return [match.value for match in matches]


def sum_key_values(nested, key):
    """Return the sum of values for key."""
    return sum(key_values(nested, key, normalize=False))


def create_index_from_source(_source, _index, _type):
    """Given an ES source dict, create ES index."""
    mappings = {}
    for k, v in _source.items():
        if type(v).__name__ in ['str', 'NoneType']:
            mappings[k] = {
                "type": "keyword"
            }
        elif isinstance(v, list):
            mappings[k] = {
                "type": "text"
            }
        else:
            # naive, there are probably other types
            mappings[k] = {"type": "float"}
    return {
        "mappings": {_type: {"properties": mappings}}
    }


def submission_client(endpoint, refresh_file):
    """Create authorized client."""
    auth = Gen3Auth(endpoint, refresh_file=refresh_file)
    assert auth, 'should return an auth client'
    submission_client_ = Gen3Submission(endpoint, auth)
    assert submission_client_, 'should return a submission client'
    assert 'delete_program' in dir(submission_client_), 'missing delete_program'
    assert 'create_program' in dir(submission_client_), 'missing create_program'
    return submission_client_


def drop_file_indexes(elastic=DEFAULT_ELASTIC, name_space=DEFAULT_NAMESPACE):
    """Drop the es indexes."""
    return {"method": 'DELETE', "url": f'{elastic}/{name_space}_file_0'}


def write_array_aliases(elastic=DEFAULT_ELASTIC, name_space=DEFAULT_NAMESPACE):
    """Write the array aliases."""
    # EXPECTED_ALIASES = {
    #     ".kibana_1": {
    #         "aliases": {
    #             ".kibana": {}
    #         }
    #     },
    #     "etl-array-config_0": {
    #         "aliases": {
    #             "etl-array-config": {},
    #             "etl_array-config": {},
    #             "time_2022-08-25T01:44:47.115494": {}
    #         }
    #     },
    #     "etl_0": {
    #         "aliases": {
    #             "etl": {},
    #             "time_2022-08-25T01:44:47.115494": {}
    #         }
    #     },
    #     "file-array-config_0": {
    #         "aliases": {
    #             "file-array-config": {},
    #             "file_array-config": {},
    #             "time_2022-08-25T01:44:47.115494": {}
    #         }
    #     },
    #     "file_0": {
    #         "aliases": {
    #             "file": {},
    #             "time_2022-08-25T01:44:47.115494": {}
    #         }
    #     }
    # }
    return {
        "method": 'POST',
        "url": f'{elastic}/_aliases',
        "json": {
            "actions": [{"add": {"index": f"{name_space}_file-array-config_0",
                                 "alias": f"{name_space}_array-config"}},
                        {"add": {"index": f"{name_space}_case-array-config_0",
                                 "alias": f"{name_space}_array-config"}},
                        {"add": {"index": f"{name_space}_file-array-config_0",
                                 "alias": f"file_array-config"}},
                        {"add": {"index": f"{name_space}_case-array-config_0",
                                 "alias": f"etl_array-config"}}
                        ]}
    }


def create_file_indexes(_source, elastic=DEFAULT_ELASTIC, name_space=DEFAULT_NAMESPACE):
    """Create the es indexes."""
    _index = f"{name_space}_file_0"
    _type = "file"
    return {
        "method": 'PUT',
        "url": f'{elastic}/{_index}',
        "json": create_index_from_source(_source, _index, _type)
    }


def write_file_array_config(elastic=DEFAULT_ELASTIC, name_space=DEFAULT_NAMESPACE):
    """Write the array config."""
    return {
        "method": 'PUT',
        "url": f'{elastic}/{name_space}_file-array-config_0/_doc/file',
        "json": {"timestamp": datetime.now().isoformat(), "array": ["project_code", "program_name"]}
    }


# gen3.aced.io_array-config gen3.aced.io_file-array-config_0 - - -

# case                      gen3.aced.io_case_0              - - -
# etl                       gen3.aced.io_case_0              - - -
# file                      gen3.aced.io_file_0              - - -

# gen3.aced.io_case         gen3.aced.io_case_0              - - -
# gen3.aced.io_file         gen3.aced.io_file_0              - - -
# etl_array-config          gen3.aced.io_case-array-config_0 - - -
# gen3.aced.io_array-config gen3.aced.io_case-array-config_0 - - -
# .kibana                   .kibana_1                        - - -


def write_file_alias_config(elastic=DEFAULT_ELASTIC, name_space=DEFAULT_NAMESPACE):
    """Write the alias config."""
    return {
        "method": 'POST',
        "url": f'{elastic}/_aliases',
        "json": {"actions": [{"add": {"index": f"{name_space}_file_0", "alias": f"file"}}]}  # {name_space}_
    }


def write_file_data(row, elastic=DEFAULT_ELASTIC, name_space=DEFAULT_NAMESPACE):
    """Write data."""
    return {
        "method": 'POST',
        "url": f'{elastic}/{name_space}_file_0/file',
        "json": row
    }


def drop_patient_indexes(elastic=DEFAULT_ELASTIC, name_space=DEFAULT_NAMESPACE):
    """Drop the es indexes."""
    return {"method": 'DELETE', "url": f'{elastic}/{name_space}_case_0'}


def create_patient_indexes(_source, elastic=DEFAULT_ELASTIC, name_space=DEFAULT_NAMESPACE):
    """Create the es indexes."""
    _index = f"{name_space}_case_0"
    _type = "case"
    return {
        "method": 'PUT',
        "url": f'{elastic}/{_index}',
        "json": create_index_from_source(_source, _index, _type)
    }


def write_patient_array_config(elastic=DEFAULT_ELASTIC, name_space=DEFAULT_NAMESPACE):
    """Write the array config."""
    # { "timestamp": "2021-09-18T17:10:49.107081", "array": [ "_file_id" ] }
    return {
        "method": 'PUT',
        "url": f'{elastic}/{name_space}_case-array-config_0/_doc/etl',
        "json": {"timestamp": datetime.now().isoformat(), "array": ['data_format', 'data_type', '_file_id']}
    }


def write_patient_alias_config(elastic=DEFAULT_ELASTIC, name_space=DEFAULT_NAMESPACE):
    """Write the alias config."""
    return {
        "method": 'POST',
        "url": f'{elastic}/_aliases',
        "json": {"actions": [{"add": {"index": f"{name_space}_case_0", "alias": f"etl"}}]}
    }


def write_patient_data(row, elastic=DEFAULT_ELASTIC, name_space=DEFAULT_NAMESPACE):
    """Write data."""
    return {
        "method": 'POST',
        "url": f'{elastic}/{name_space}_case_0/case',
        "json": row
    }


def read_files(sc, batch_size):
    """Read file records and their ancestors from gen3, map to elastic search."""
    first = batch_size
    offset = 0
    c = 0
    while True:
        graphql_variables = {"first": first, "offset": offset}
        graph_ql_response = sc.query(FILE_GRAPHQL, variables=graphql_variables)
        c += 1
        has_data = False

        if 'data' in graph_ql_response:
            sum_data = 0
            for k in graph_ql_response['data']:
                sum_data += len(graph_ql_response['data'][k])
            has_data = sum_data > 0
        if not has_data:
            break
        offset += batch_size

        # we get patient with array of document_references
        for patient in graph_ql_response['data']['patient']:
            assert 'project_id' in patient, (patient, patient.keys())
            program, project = patient['project_id'].split('-')
            for document_reference in patient['document_references']:
                document_reference["auth_resource_path"] = f"/programs/{program}/projects/{project}"
                for k, v in patient.items():
                    if k == 'document_references':
                        continue
                    document_reference[f"patient_{k}"] = v

                    # map fields hard coded by windmill portal
                    # data_types & data_format
                    if document_reference['content_0_attachment_url']:
                        document_reference['data_type'] = document_reference['content_0_attachment_url'].split('.')[-1]
                    else:
                        document_reference['data_type'] = 'txt'

                    if document_reference['data_type'] in ['csv']:
                        document_reference['data_format'] = 'variants'
                    if document_reference['data_type'] in ['dcm']:
                        document_reference['data_format'] = 'imaging'
                    if document_reference['data_type'] in ['txt']:
                        document_reference['data_format'] = 'note'

                    document_reference['file_name'] = document_reference['content_0_attachment_url']
                    document_reference['file_size'] = 0
                    if document_reference['content_0_attachment_size']:
                        document_reference['file_size'] = int(document_reference['content_0_attachment_size'])

                yield document_reference


def read_patients(sc, batch_size):
    """Read patient records and their descendants from gen3."""
    first = batch_size
    offset = 0
    c = 0
    while True:
        # pagination
        graphql_variables = {"first": first, "offset": offset}
        r = sc.query(PATIENT_GRAPHQL, variables=graphql_variables)
        c += 1

        if 'data' not in r or 'patient' not in r['data'] or len(r['data']['patient']) == 0:
            break
        offset += batch_size
        for patient in r['data']['patient']:
            assert 'project_id' in patient, (patient, patient.keys())
            program, project = patient['project_id'].split('-')
            patient["auth_resource_path"] = f"/programs/{program}/projects/{project}"
            patient["_case_id"] = patient['id']  # TODO ??? in gitops ?
            yield patient

            # _source = {
            #     "project_id": patient['project_id'],
            #     "patient_id": patient['patient_id'],
            #     "patient_resource_type": patient['patient_resource_type'],
            #     # "text_status": patient['text_status'],
            #     # "text_div": patient['text_div'],
            #
            #     # TODO - review extension anonymization
            #     # "us_core_race_ombCategory": patient['us_core_race_ombCategory'],
            #     # "us_core_race_text": patient['us_core_race_text'],
            #     # "us_core_ethnicity_ombCategory": patient['us_core_ethnicity_ombCategory'],
            #     # "us_core_ethnicity_text": patient['us_core_ethnicity_text'],
            #     # "patient_mothersMaidenName": patient['patient_mothersMaidenName'],
            #     # "us_core_birthsex": patient['us_core_birthsex'],
            #
            #     # TODO - more extension
            #     # "patient_birthPlace_city": patient['patient_birthPlace_city'],
            #     # "patient_birthPlace_state": patient['patient_birthPlace_state'],
            #     # "patient_birthPlace_country": patient['patient_birthPlace_country'],
            #
            #     # TODO - more extension
            #     # "disability_adjusted_life_years": patient['disability_adjusted_life_years'],
            #     # "quality_adjusted_life_years": patient['quality_adjusted_life_years'],
            #     # "identifier_synthea": patient['identifier_synthea'],
            #     # "identifier_MR": patient['identifier_MR'],
            #     # "identifier_SS": patient['identifier_SS'],
            #     # "identifier_DL": patient['identifier_DL'],
            #     # "identifier_PPN": patient['identifier_PPN'],
            #
            #     # TODO - anonymizer
            #     # "name_use": patient['name_use'],
            #     # "name_family": patient['name_family'],
            #     # "name_given": patient['name_given'],
            #     # "name_prefix": patient['name_prefix'],
            #     # "telecom_system": patient['telecom_system'],
            #     # "telecom_value": patient['telecom_value'],
            #     # "telecom_use": patient['telecom_use'],
            #
            #     "gender": patient['gender'],
            #     "birthDate": patient['birthDate'],
            #     "deceasedDateTime": patient['deceasedDateTime'],
            #     "deceasedBoolean": patient['deceasedBoolean'],
            #
            #     # TODO - anonymizer
            #     # "address_geolocation_latitude": patient['address_geolocation_latitude'],
            #     # "address_geolocation_longitude": patient['address_geolocation_longitude'],
            #     # "address_line": patient['address_line'],
            #     # "address_city": patient['address_city'],
            #     # "address_state": patient['address_state'],
            #     # "address_postalCode": patient['address_postalCode'],
            #     # "address_country": patient['address_country'],
            #
            #     # "maritalStatus_coding_0_code": patient['maritalStatus_coding_0_code'],
            #     # "maritalStatus_coding_0_system": patient['maritalStatus_coding_0_system'],
            #     "maritalStatus_coding_0_display": patient['maritalStatus_coding_0_display'],
            #
            #     # "multipleBirthBoolean": str(patient['multipleBirthBoolean']),   # TODO handle booleans better
            #     # "multipleBirthInteger": patient['multipleBirthInteger'],
            #
            #     # "communication_language_text": patient['communication_language_text'],
            #     "patient_submitter_id": patient['submitter_id'],
            #     "auth_resource_path": f"/programs/{program}/projects/{project}",
            #
            #     "_case_id": patient['patient_id'],  # TODO ??? in gitops ?
            #
            #     "file_id": key_values(patient, 'file_id'),
            #     "author_display": key_values(patient, 'author_display'),
            #     "author_reference": key_values(patient, 'author_reference'),
            #     # "category_coding_us_core_documentreference_category": key_values(patient, 'category_coding_us_core_documentreference_category'),
            #
            #     "content_attachment_contentType": key_values(patient, 'content_0_attachment_contentType'),
            #     "content_attachment_md5": key_values(patient, 'content_0_attachment_md5'),
            #     "content_attachment_size": key_values(patient, 'content_0_attachment_size'),
            #     "content_attachment_url": key_values(patient, 'content_0_attachment_url'),
            #
            #     "content_format_code": key_values(patient, 'content_0_format_code'),
            #     "content_format_system": key_values(patient, 'content_0_format_system'),
            #     "content_format_display": key_values(patient, 'content_0_format_display'),
            #
            #     "context_encounter_reference": key_values(patient, 'context_encounter_0_reference'),
            #     "context_period_end": key_values(patient, 'context_period_end'),
            #     "context_period_start": key_values(patient, 'context_period_start'),
            #
            #     # "custodian_display": key_values(patient, 'custodian_display'),
            #     # "custodian_reference": key_values(patient, 'custodian_reference'),
            #     #
            #     "date": key_values(patient, 'date'),
            #     "md5sum": key_values(patient, 'md5sum'),
            #     "ga4gh_drs_uri": key_values(patient, 'ga4gh_drs_uri'),
            #     # "identifier_urn_ietf_rfc_3986": key_values(patient, 'identifier_urn_ietf_rfc_3986'),
            #
            #     "file_resourceType": key_values(patient, 'resource_type'),
            #     "status": key_values(patient, 'status'),
            #     # "file_submitter_id": key_values(patient, 'file_submitter_id'),
            #     # "type_coding_0_code": key_values(patient, 'type_coding_0_code'),
            #     # "type_coding_0_display": key_values(patient, 'type_coding_0_display'),
            #     # "type_coding_0_system": key_values(patient, 'type_coding_0_system'),
            #     # "type_coding_1_code": key_values(patient, 'type_coding_1_code'),
            #     # "type_coding_1_system": key_values(patient, 'type_coding_1_system'),
            #     # "type_coding_1_display": key_values(patient, 'type_coding_1_display'),
            #     "data_format": key_values(patient, 'data_format'),
            #     "data_type": key_values(patient, 'data_type')
            #
            #
            # }
            #
            # yield _source


def write_dict(output, d):
    """Write a dict to the output."""
    output.write(str(d))
    output.write("\n")


def write_http(session, _params, raise_for_status=True):
    """Write a dict to the session."""
    r = session.request(**_params)
    if raise_for_status:
        if r.status_code > 300:
            print("TEXT_PARAMS", _params)
            print("TEXT", r.text)
        r.raise_for_status()


@click.command()
@click.option('--endpoint', type=str, help='Gen3 host base url.')
@click.option('--credentials_path', type=str, help='Path to gen3 credentials.')
@click.option('--batch_size', type=int, default=500, help='Number of records to read from gen3 at a time (500.')
@click.option('--output_path', type=str, default=None, help='For debugging, write the output to this path')
@click.option('--elastic', type=str, default=None, help='Write directly to elastic host')
@click.option('--limit',
              default=None,
              show_default=True,
              help='Max number of rows per index.')
def etl(credentials_path, endpoint, output_path, batch_size, elastic, limit):
    """Extract file centric index from Gen3, create elastic search index."""
    # check destination
    assert output_path or elastic, "Please set either elastic url or output_path file path"
    # connect to source (gen3)
    sc = submission_client(endpoint, credentials_path)

    if limit:
        limit = int(limit)
    # create a handy little function that writes to either file or session
    output_stream = None
    write_method = None
    if output_path:
        output_stream = open(output_path, "w")
        write_method = write_dict
    else:
        output_stream = requests.sessions.Session()
        write_method = write_http

    def _writer(data):
        """Write to destination"""
        write_method(output_stream, data)

    #
    # FILE centric index
    #

    # assumes guppy-setup dropped ES indices
    #_writer(drop_file_indexes(elastic))

    # write data

    print(f'Reading files. batch_size {batch_size}')
    for i, row in enumerate(read_files(sc, batch_size)):
        if i == 0:
            print('Creating file indices.')
            _writer(create_file_indexes(row, elastic))
        _writer(write_file_data(row, elastic))
        if i % batch_size == 0:
            print(f'Wrote {i} files')
        if limit and i > limit:
            break  # for testing
    _writer(write_file_array_config(elastic))
    _writer(write_file_alias_config(elastic))
    _writer(write_file_array_config(elastic))

    #
    # PATIENT centric index
    #
    print('Reading patients.')
    for i, row in enumerate(read_patients(sc, batch_size)):
        if i == 0:
            print('Creating patient indices.')
            _writer(create_patient_indexes(row, elastic))
        _writer(write_patient_data(row, elastic))
        if i % batch_size == 0:
            print(f'Wrote {i} patients')
        if limit and i > limit:
            break  # for testing

    _writer(write_patient_array_config(elastic))
    _writer(write_patient_alias_config(elastic))
    _writer(write_array_aliases(elastic))

    # cleanup
    output_stream.close()


if __name__ == '__main__':
    etl()
