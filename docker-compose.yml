version: '3'
services:
  postgres:
    image: postgres:9.6
    container_name: postgres
    networks:
      - devnet
    volumes:
      - psqldata:/var/lib/postgresql/data
      - ./scripts/postgres_init.sql:/docker-entrypoint-initdb.d/postgres_init.sql:ro
      - ./scripts/postgres_always.sh:/postgres_always.sh:ro
      - ./scripts/postgres_run.sh:/usr/local/bin/postgres_run.sh:ro
    restart: unless-stopped
    logging:
      driver: "json-file"
      options:
        max-size: "10m"      
    healthcheck:
      test: ["CMD-SHELL", "psql -U fence_user -d fence_db -c 'SELECT 1;'"]
      interval: 60s
      timeout: 5s
      retries: 3
    command: postgres_run.sh
    environment:
      - POSTGRES_PASSWORD=postgres
  indexd-service:
    image: "quay.io/cdis/indexd:2022.09"
    command: bash indexd_setup.sh
    container_name: indexd-service
    networks:
      - devnet
    environment:
      # https://github.com/uc-cdis/indexd/blob/b1eb4824eecf668b82bafe47304d772526d96d10/indexd/app.py#L27
      - PRESIGNED_FENCE_URL=http://fence-service
    volumes:
      - ./Secrets/indexd_settings.py:/var/www/indexd/local_settings.py
      - ./Secrets/indexd_creds.json:/var/www/indexd/creds.json
      - ./Secrets/config_helper.py:/var/www/indexd/config_helper.py
      - ./scripts/indexd_setup.sh:/var/www/indexd/indexd_setup.sh
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost/_status"]
      interval: 60s
      timeout: 5s
      retries: 3
    depends_on:
      - postgres
    restart: unless-stopped
    logging:
      driver: "json-file"
      options:
        max-size: "10m"        
  fence-service:
    image: "quay.io/cdis/fence:integration202211"
    #build: fence
    command: bash /var/www/fence/fence_setup.sh
    container_name: fence-service
    networks:
      - devnet
    volumes:
      - ./Secrets/fence-config.yaml:/var/www/fence/fence-config.yaml
      - ./Secrets/user.yaml:/var/www/fence/user.yaml
      - ./Secrets/TLS/service.crt:/usr/local/share/ca-certificates/cdis-ca.crt
      - ./Secrets/fenceJwtKeys:/fence/keys
      - ./scripts/fence_setup.sh:/var/www/fence/fence_setup.sh
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost/_status"]
      interval: 60s
      timeout: 5s
      retries: 3
    environment:
      - PYTHONPATH=/var/www/fence
      - GEN3_DEBUG=True
    depends_on:
      - postgres
      - arborist-service
    restart: unless-stopped
    logging:
      driver: "json-file"
      options:
        max-size: "10m"        
  arborist-service:
    image: "quay.io/cdis/arborist:2021.03"
    container_name: arborist-service
    entrypoint: bash /go/src/github.com/uc-cdis/arborist/arborist_setup.sh
    networks:
      - devnet
    volumes:
      - ./scripts/arborist_setup.sh:/go/src/github.com/uc-cdis/arborist/arborist_setup.sh
    environment:
      - JWKS_ENDPOINT=http://fence-service/.well-known/jwks
      - PGDATABASE=arborist_db
      - PGUSER=arborist_user
      - PGPASSWORD=arborist_pass
      - PGHOST=postgres
      - PGPORT=5432
      - PGSSLMODE=disable
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost/health"]
      interval: 60s
      timeout: 5s
      retries: 10
    depends_on:
      - postgres
    restart: unless-stopped
    logging:
      driver: "json-file"
      options:
        max-size: "10m"        
  metadata-service:
    # update image
    image: "quay.io/cdis/metadata-service:1.8.0"
    container_name: metadata-service
    depends_on:
      - postgres
    volumes:
      # To load:
      # scripts/discovery_metadata.sh
      # Also interesting:
      # /env/bin/python /src/src/mds/populate.py --config /var/local/metadata-service/aggregate_config.json
      - ./Secrets/metadata/aggregate_config.json:/var/local/metadata-service/aggregate_config.json
    environment:
      - DB_HOST=postgres
      - DB_USER=metadata_user
      - DB_PASSWORD=metadata_pass
      - DB_DATABASE=metadata
      - DEBUG=true
      - USE_AGG_MDS=false
      - AGG_MDS_NAMESPACE=default_namespace
      - GEN3_ES_ENDPOINT=http://esproxy-service:9200
    command: >
      sh -c "/env/bin/alembic upgrade head && /env/bin/uvicorn --host 0.0.0.0 --port 80 mds.asgi:app --reload"
    networks:
      - devnet
    restart: unless-stopped
    logging:
      driver: "json-file"
      options:
        max-size: "10m"        
  peregrine-service:
    image: "quay.io/cdis/peregrine:2021.03"
    container_name: peregrine-service
    networks:
      - devnet
    volumes:
      - ./Secrets/peregrine_settings.py:/var/www/peregrine/wsgi.py
      - ./Secrets/peregrine_creds.json:/var/www/peregrine/creds.json
      - ./Secrets/config_helper.py:/var/www/peregrine/config_helper.py
      - ./Secrets/TLS/service.crt:/usr/local/share/ca-certificates/cdis-ca.crt
      - ./scripts/peregrine_setup.sh:/peregrine_setup.sh
      - ./datadictionary/gdcdictionary/schemas:/schemas_dir
    environment: &env
      DICTIONARY_URL: https://aced-public.s3.us-west-2.amazonaws.com/aced-test.json
      # PATH_TO_SCHEMA_DIR: /schemas_dir
      REQUESTS_CA_BUNDLE: /etc/ssl/certs/ca-certificates.crt
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost/_status"]
      interval: 60s
      timeout: 5s
      # give peregrine some extra time to startup
      retries: 10
    depends_on:
      - postgres
      - sheepdog-service
    restart: unless-stopped
    logging:
      driver: "json-file"
      options:
        max-size: "10m"        
  sheepdog-service:
    image: "quay.io/cdis/sheepdog:2021.03"
    command: bash /sheepdog_setup.sh
    container_name: sheepdog-service
    networks:
      - devnet
    volumes:
      - ./Secrets/sheepdog_settings.py:/var/www/sheepdog/wsgi.py
      - ./Secrets/sheepdog_creds.json:/var/www/sheepdog/creds.json
      - ./Secrets/config_helper.py:/var/www/sheepdog/config_helper.py
      - ./scripts/sheepdog_setup.sh:/sheepdog_setup.sh
      - ./datadictionary/gdcdictionary/schemas:/schemas_dir
    environment: *env
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost/_status"]
      interval: 60s
      timeout: 5s
      retries: 5
    depends_on:
      - postgres
    restart: unless-stopped
    logging:
      driver: "json-file"
      options:
        max-size: "10m"        
  guppy-service:
    image: "quay.io/cdis/guppy:2021.03"
    container_name: guppy-service
    networks:
      - devnet
    volumes:
      - ./Secrets/guppy_config.json:/guppy/guppy_config.json
      - ./scripts/wait_for_esproxy.sh:/usr/bin/wait_for_esproxy.sh:ro
    entrypoint: /usr/bin/wait_for_esproxy.sh
    command: node --max-http-header-size 16000 dist/server/server.js
    environment:
      - GUPPY_CONFIG_FILEPATH=/guppy/guppy_config.json
      - GEN3_ARBORIST_ENDPOINT=http://arborist-service
      - GEN3_ES_ENDPOINT=http://esproxy-service:9200
    depends_on:
      - arborist-service
      - esproxy-service
    restart: unless-stopped
    logging:
      driver: "json-file"
      options:
        max-size: "10m"        
  esproxy-service:
    image: quay.io/cdis/elasticsearch-oss:6.8.12
    container_name: esproxy-service
    environment:
      - cluster.name=elasticsearch-cluster
      - bootstrap.memory_lock=false
      # For apple silicon
      - bootstrap.system_call_filter=false
      - "ES_JAVA_OPTS=-Xms4g -Xmx4g"
    entrypoint:
      - /bin/bash
    # mmapfs requires systemctl update - see https://www.elastic.co/guide/en/elasticsearch/reference/current/index-modules-store.html#mmapfs
    command:
      - -c
      - "echo -e 'cluster.name: docker-cluster\nhttp.host: 0.0.0.0\nindex.store.type: niofs' > /usr/share/elasticsearch/config/elasticsearch.yml && /usr/local/bin/docker-entrypoint.sh eswrapper"
    volumes:
      - ./scripts/wait_for_esproxy.sh:/usr/bin/wait_for_esproxy.sh:ro
      - esdata:/usr/share/elasticsearch/data
    healthcheck:
      test: ["CMD-SHELL", "wait_for_esproxy.sh"]
      interval: 60s
      timeout: 5s
      # give peregrine some extra time to startup
      retries: 10
    ulimits:
      memlock:
        soft: -1
        hard: -1
      nofile:
        soft: 65536
        hard: 65536
    ports:
      - 9200:9200
      - 9300:9300
    networks:
      - devnet
    restart: unless-stopped
    logging:
      driver: "json-file"
      options:
        max-size: "10m"        
  pidgin-service:
    image: "quay.io/cdis/pidgin:2021.03"
    container_name: pidgin-service
    networks:
      - devnet
    volumes:
      - ./scripts/waitForContainers.sh:/var/www/data-portal/waitForContainers.sh
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost/_status"]
      interval: 60s
      timeout: 5s
      retries: 3
    depends_on:
      - peregrine-service
    restart: unless-stopped
    logging:
      driver: "json-file"
      options:
        max-size: "10m"        
  portal-service:
    image: "quay.io/cdis/data-portal:5.1.0"  # 3.33.0"  # 2021.03
    container_name: portal-service
    command: ["bash", "/var/www/data-portal/waitForContainers.sh"]
    deploy:
        resources:
            reservations:
              cpus: "3.0"
    networks:
      - devnet
    volumes:
      - ./scripts/waitForContainers.sh:/var/www/data-portal/waitForContainers.sh
      - ./Secrets/gitops.json:/data-portal/data/config/gitops.json
      - ./Secrets/gitops-logo.png:/data-portal/custom/logo/gitops-logo.png
      - ./Secrets/gitops.png:/data-portal/custom/createdby/gitops.png
      #  BRH biomedical research hub graphics
      # - ./Secrets/gitops-sponsors/createdby_50.png:/data-portal/custom/sponsors/gitops-sponsors/createdby_50.png
      # - ./Secrets/gitops-sponsors/gen3_50.png:/data-portal/custom/sponsors/gitops-sponsors/gen3_50.png
      # - ./Secrets/gitops-sponsors/gene_bgy.svg:/data-portal/custom/sponsors/gitops-sponsors/gene_bgy.svg
    environment:
      - NODE_ENV=dev
      #- MOCK_STORE=true
      - APP=gitops
      - GDC_SUBPATH=https://revproxy-service/api/v0/submission/
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost"]
      interval: 60s
      timeout: 5s
      retries: 10
    depends_on:
      - postgres
      - peregrine-service
      - sheepdog-service
    restart: unless-stopped
    logging:
      driver: "json-file"
      options:
        max-size: "10m"        
  jupyter-service:
    image: "quay.io/cdis/jupyter-slim:latest"
    container_name: jupyter-service
    entrypoint:
      - "start-notebook.sh"
    command:
      - "--NotebookApp.base_url=/lw-workspace/proxy"
      - "--NotebookApp.password=''"
      - "--NotebookApp.token=''"
    networks:
      - devnet
    environment:
      - FRAME_ANCESTORS=http://staging.aced-idp.org
    restart: unless-stopped
    logging:
      driver: "json-file"
      options:
        max-size: "10m"        
  revproxy-service:
    image: "quay.io/cdis/nginx:2021.03"
    container_name: revproxy-service
    networks:
      - devnet
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf
      - ./minio.conf.staging:/etc/nginx/minio.conf
      - ./Secrets/TLS/service.crt:/etc/nginx/ssl/nginx.crt
      - ./Secrets/TLS/service.key:/etc/nginx/ssl/nginx.key
    ports:
      - "80:80"
      - "443:443"
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost/_status"]
      interval: 60s
      timeout: 5s
      retries: 3
    depends_on:
      - arborist-service
      - indexd-service
      - peregrine-service
      - sheepdog-service
      - fence-service
      - portal-service
      - pidgin-service
    restart: unless-stopped
    logging:
      driver: "json-file"
      options:
        max-size: "10m"        
  kibana-service:
    image: quay.io/cdis/kibana-oss:6.5.4
    container_name: kibana-service
    environment:
      - SERVER_NAME=kibana-service
      - ELASTICSEARCH_URL=http://esproxy-service:9200
    ports:
      - 5601:5601
    networks:
      - devnet
    depends_on:
      - esproxy-service
networks:
  devnet:
volumes:
  psqldata:
  esdata:
